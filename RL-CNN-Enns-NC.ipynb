{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers, losses\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.signal import windows\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "from typing import Tuple, Union, Optional, List, Dict\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_DIM = 6\n",
    "N_ACTIONS = 5\n",
    "EPSILON = 0.3\n",
    "ALPHA1 = 0.99\n",
    "ALPHA2 = 0.996\n",
    "M_MIN_MAX = 10\n",
    "Q_NET_ARCH = [16, 16]\n",
    "RESCALE_RANGE = (0.1, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLEnvironment & Epsilon-Greedy Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers, losses\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.signal import windows\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "from typing import Tuple, Union, Optional, List, Dict\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "class RLEnvironment:\n",
    "    def __init__(self, optimizer: tf.keras.optimizers.Optimizer,\n",
    "                 initial_lr: float,\n",
    "                 m_min_max: int = M_MIN_MAX,\n",
    "                 alpha1: float = ALPHA1,\n",
    "                 alpha2: float = ALPHA2):\n",
    "        if not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n",
    "            log.warning(\"Optimizer might not be a standard Keras optimizer.\")\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = float(initial_lr)\n",
    "        self.m_min_max = m_min_max\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.iteration = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "        self.lowest_losses = []\n",
    "        self.prev_gradient_flat = None\n",
    "        self._current_lr_float = self.initial_lr\n",
    "        self._update_optimizer_lr(self._current_lr_float)\n",
    "        log.info(f\"RLEnvironment initialized with initial_lr={self.initial_lr}, \"\n",
    "                 f\"alpha1={self.alpha1}, alpha2={self.alpha2}, M={self.m_min_max}\")\n",
    "\n",
    "    def _update_optimizer_lr(self, new_lr_float: float):\n",
    "        self._current_lr_float = new_lr_float\n",
    "        try:\n",
    "            self.optimizer.learning_rate.assign(new_lr_float)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                self.optimizer.learning_rate = new_lr_float\n",
    "            except Exception as e:\n",
    "                log.error(f\"Failed to set optimizer learning rate: {e}. LR control might not work correctly.\")\n",
    "\n",
    "    def _update_lowest_losses(self, loss_value: float):\n",
    "        if len(self.lowest_losses) < self.m_min_max:\n",
    "            self.lowest_losses.append(loss_value)\n",
    "            self.lowest_losses.sort()\n",
    "        elif loss_value < self.lowest_losses[-1]:\n",
    "            self.lowest_losses.pop()\n",
    "            self.lowest_losses.append(loss_value)\n",
    "            self.lowest_losses.sort()\n",
    "\n",
    "    def get_current_lr(self) -> float:\n",
    "        return self._current_lr_float\n",
    "\n",
    "    def calculate_state(self, current_loss: tf.Tensor, current_gradient_list: list) -> np.ndarray:\n",
    "        lr_state = self._current_lr_float\n",
    "        loss_state = float(current_loss.numpy())\n",
    "        if current_gradient_list and any(g is not None for g in current_gradient_list):\n",
    "            flat_gradients = tf.concat([\n",
    "                tf.reshape(g, [-1]) for g in current_gradient_list if g is not None], axis=0)\n",
    "            grad_norm_state = float(tf.norm(flat_gradients).numpy())\n",
    "            current_gradient_flat_np = flat_gradients.numpy()\n",
    "        else:\n",
    "            log.warning(\"No valid gradients found for state calculation.\")\n",
    "            grad_norm_state = 0.0\n",
    "            current_gradient_flat_np = None\n",
    "        iter_state = int(self.iteration.numpy())\n",
    "        min_max_enc_state = 0\n",
    "        if self.lowest_losses:\n",
    "            min_loss_tracked = self.lowest_losses[0]\n",
    "            max_loss_tracked = self.lowest_losses[-1]\n",
    "            if loss_state <= min_loss_tracked:\n",
    "                min_max_enc_state = 1\n",
    "            elif loss_state > max_loss_tracked:\n",
    "                min_max_enc_state = -1\n",
    "        self._update_lowest_losses(loss_state)\n",
    "        align_state = 0.0\n",
    "        if (self.prev_gradient_flat is not None and\n",
    "            current_gradient_flat_np is not None and\n",
    "            current_gradient_flat_np.shape == self.prev_gradient_flat.shape):\n",
    "            sign_prod = np.sign(current_gradient_flat_np * self.prev_gradient_flat)\n",
    "            sign_prod = np.nan_to_num(sign_prod, nan=0.0)\n",
    "            align_state = float(np.mean(sign_prod))\n",
    "        elif self.prev_gradient_flat is None and iter_state > 0:\n",
    "            log.warning(\"Previous gradient missing for alignment calculation.\")\n",
    "        elif current_gradient_flat_np is None:\n",
    "            log.warning(\"Current gradient missing for alignment calculation.\")\n",
    "        elif (self.prev_gradient_flat is not None and\n",
    "              current_gradient_flat_np is not None and\n",
    "              current_gradient_flat_np.shape != self.prev_gradient_flat.shape):\n",
    "            log.error(\"Gradient shape mismatch for alignment calculation. This should not happen.\")\n",
    "        if current_gradient_flat_np is not None:\n",
    "            self.prev_gradient_flat = current_gradient_flat_np\n",
    "        self.iteration.assign_add(1)\n",
    "        state = np.array([\n",
    "            lr_state, loss_state, grad_norm_state,\n",
    "            iter_state, min_max_enc_state, align_state\n",
    "        ], dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "    def apply_action_and_get_reward(self, action: int, current_loss: tf.Tensor) -> tuple[float, float]:\n",
    "        current_lr = self._current_lr_float\n",
    "        if action == 0:\n",
    "            new_lr = current_lr / self.alpha1\n",
    "        elif action == 1:\n",
    "            new_lr = current_lr / self.alpha2\n",
    "        elif action == 2:\n",
    "            new_lr = current_lr\n",
    "        elif action == 3:\n",
    "            new_lr = current_lr * self.alpha2\n",
    "        elif action == 4:\n",
    "            new_lr = current_lr * self.alpha1\n",
    "        else:\n",
    "            log.error(f\"Invalid action received: {action}. Keeping LR unchanged.\")\n",
    "            new_lr = current_lr\n",
    "        self._update_optimizer_lr(new_lr)\n",
    "        loss_float = float(current_loss.numpy())\n",
    "        reward = 1.0 / (loss_float + 1e-9)\n",
    "        return self.get_current_lr(), reward\n",
    "\n",
    "    def reset(self):\n",
    "        log.info(\"Resetting RLEnvironment state.\")\n",
    "        self.iteration.assign(0)\n",
    "        self.lowest_losses = []\n",
    "        self.prev_gradient_flat = None\n",
    "        self._update_optimizer_lr(self.initial_lr)\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, n_actions: int = N_ACTIONS, epsilon: float = EPSILON):\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        log.info(f\"EpsilonGreedyPolicy initialized with n_actions={n_actions}, epsilon={epsilon}\")\n",
    "\n",
    "    def select_action(self, q_values: tf.Tensor) -> int:\n",
    "        q_values_squeezed = tf.squeeze(q_values).numpy()\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            return int(np.argmax(q_values_squeezed))\n",
    "\n",
    "    def select_action_eq8(self, q_values: tf.Tensor) -> int:\n",
    "        q_values_squeezed = tf.squeeze(q_values).numpy()\n",
    "        best_action = np.argmax(q_values_squeezed)\n",
    "        probabilities = np.full(self.n_actions, self.epsilon / self.n_actions)\n",
    "        probabilities[best_action] += (1.0 - self.epsilon)\n",
    "        prob_sum = np.sum(probabilities)\n",
    "        if not np.isclose(prob_sum, 1.0):\n",
    "            probabilities /= prob_sum\n",
    "        return int(np.random.choice(self.n_actions, p=probabilities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Network & Rescaler & DDQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers, losses\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.signal import windows\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "from typing import Tuple, Union, Optional, List, Dict\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "class QNetwork(Model):\n",
    "    def __init__(self, state_dim: int = STATE_DIM, action_dim: int = N_ACTIONS, hidden_units: list = Q_NET_ARCH):\n",
    "        super().__init__(name='QNetwork')\n",
    "        self.layers_list = [layers.Dense(units, activation='relu') for units in hidden_units]\n",
    "        self.layers_list.append(layers.Dense(action_dim, activation='sigmoid'))\n",
    "        self.build(input_shape=(None, state_dim))\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
    "        for layer in self.layers_list:\n",
    "            try:\n",
    "                x = layer(x, training=training)\n",
    "            except TypeError:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Rescaler:\n",
    "    def __init__(self, low: float = RESCALE_RANGE[0], high: float = RESCALE_RANGE[1], epsilon: float = 1e-6):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.range = high - low\n",
    "        self.epsilon = epsilon\n",
    "        self.running_min = None\n",
    "        self.running_max = None\n",
    "        self.alpha = 0.01\n",
    "\n",
    "    def fit(self, y: np.ndarray):\n",
    "        if y is None or y.size == 0:\n",
    "            return\n",
    "        if hasattr(y, 'numpy'):\n",
    "            y = y.numpy()\n",
    "        vals = y[np.isfinite(y)]\n",
    "        if vals.size == 0:\n",
    "            return\n",
    "        mn, mx = np.min(vals), np.max(vals)\n",
    "        if self.running_min is None or not np.isfinite(self.running_min):\n",
    "            self.running_min, self.running_max = mn, mx\n",
    "        else:\n",
    "            self.running_min = self.alpha * mn + (1 - self.alpha) * self.running_min\n",
    "            self.running_max = self.alpha * mx + (1 - self.alpha) * self.running_max\n",
    "        if self.running_min > self.running_max:\n",
    "            self.running_min, self.running_max = mn, mx\n",
    "\n",
    "    def scale(self, y: np.ndarray) -> np.ndarray:\n",
    "        if hasattr(y, 'numpy'):\n",
    "            y = y.numpy()\n",
    "        if self.running_min is None or self.running_max is None:\n",
    "            return np.full_like(y, (self.low + self.high) / 2.0, dtype=np.float32)\n",
    "        rng = self.running_max - self.running_min\n",
    "        if rng < self.epsilon:\n",
    "            return np.full_like(y, (self.low + self.high) / 2.0, dtype=np.float32)\n",
    "        mask = np.isfinite(y)\n",
    "        out = np.full_like(y, (self.low + self.high) / 2.0, dtype=np.float32)\n",
    "        out[mask] = self.low + self.range * (y[mask] - self.running_min) / rng\n",
    "        return np.clip(out, self.low, self.high).astype(np.float32)\n",
    "\n",
    "    def inv_scale(self, y: np.ndarray) -> np.ndarray:\n",
    "        if hasattr(y, 'numpy'):\n",
    "            y = y.numpy()\n",
    "        if self.running_min is None or self.running_max is None:\n",
    "            return y.astype(np.float32)\n",
    "        rng = self.running_max - self.running_min\n",
    "        if rng < self.epsilon:\n",
    "            return np.full_like(y, self.running_min, dtype=np.float32)\n",
    "        mask = np.isfinite(y)\n",
    "        out = np.full_like(y, self.running_min, dtype=np.float32)\n",
    "        out[mask] = self.running_min + (y[mask] - self.low) * rng / self.range\n",
    "        return out.astype(np.float32)\n",
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(\n",
    "        self, state_dim: int = STATE_DIM,\n",
    "        action_dim: int = N_ACTIONS,\n",
    "        buffer_size: int = 0,\n",
    "        batch_size: int = 0,\n",
    "        gamma: float = 0.0,\n",
    "        q_learning_rate: float = 0.0,\n",
    "        target_update_freq: int = 0\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.main_net = QNetwork(state_dim, action_dim)\n",
    "        self.target_net = QNetwork(state_dim, action_dim)\n",
    "        self.target_net.build(input_shape=(None, state_dim))\n",
    "        self.main_net.build(input_shape=(None, state_dim))\n",
    "        self.target_net.set_weights(self.main_net.get_weights())\n",
    "        self.optimizer = optimizers.Adam(learning_rate=q_learning_rate)\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.rescaler = Rescaler()\n",
    "        self.loss_fn = losses.MeanSquaredError()\n",
    "        self._update_counter = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        transition = (np.array(state, dtype=np.float32), int(action), float(reward), np.array(next_state, dtype=np.float32), bool(done))\n",
    "        self.replay_buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        s, a, r, ns, d = map(np.array, zip(*batch))\n",
    "        return s.astype(np.float32), a.astype(np.int32), r.astype(np.float32), ns.astype(np.float32), d.astype(np.bool_)\n",
    "\n",
    "    @tf.function\n",
    "    def _train_target_step(self, states, actions, rewards, next_states, dones):\n",
    "        q_main_next = self.main_net(next_states, training=False)\n",
    "        best_next = tf.argmax(q_main_next, axis=1, output_type=tf.int32)\n",
    "        q_target_next = tf.gather_nd(self.target_net(next_states, training=False), tf.stack([tf.range(tf.shape(actions)[0]), best_next], axis=1))\n",
    "        inv_q = tf.numpy_function(self.rescaler.inv_scale, [q_target_next], tf.float32)\n",
    "        inv_q = tf.reshape(inv_q, [-1])\n",
    "        rewards = tf.reshape(rewards, [-1])\n",
    "        dones_f = tf.cast(tf.reshape(dones, [-1]), tf.float32)\n",
    "        y = rewards + self.gamma * inv_q * (1 - dones_f)\n",
    "        y_scaled = tf.numpy_function(self.rescaler.scale, [y], tf.float32)\n",
    "        y_scaled = tf.reshape(y_scaled, [-1])\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_curr = tf.gather_nd(self.target_net(states, training=True), tf.stack([tf.range(tf.shape(actions)[0]), actions], axis=1))\n",
    "            loss = self.loss_fn(y_scaled, q_curr)\n",
    "        grads = tape.gradient(loss, self.target_net.trainable_variables)\n",
    "        self.optimizer.apply_gradients([(g,v) for g,v in zip(grads, self.target_net.trainable_variables) if g is not None])\n",
    "        return loss, y\n",
    "\n",
    "    def update(self):\n",
    "        batch = self.sample_batch()\n",
    "        if batch is None:\n",
    "            return None\n",
    "        s, a, r, ns, d = batch\n",
    "        loss, y = self._train_target_step(s, a, r, ns, d)\n",
    "        self.rescaler.fit(y.numpy())\n",
    "        self._update_counter.assign_add(1)\n",
    "        if self._update_counter % self.target_update_freq == 0:\n",
    "            self.main_net.set_weights(self.target_net.get_weights())\n",
    "        return float(loss.numpy())\n",
    "\n",
    "    @tf.function\n",
    "    def get_q_values(self, state_tensor):\n",
    "        return self.main_net(state_tensor, training=False)\n",
    "\n",
    "    def get_action(self, state: np.ndarray, policy) -> int:\n",
    "        q = self.get_q_values(tf.convert_to_tensor(state.reshape(1, -1), tf.float32))\n",
    "        return policy.select_action(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers, losses\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.signal import windows\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "from typing import Tuple, Union, Optional, List, Dict\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@tf.function\n",
    "def cnn_train_step(cnn_model, optimizer, loss_fn, x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = cnn_model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, preds)\n",
    "        if cnn_model.losses:\n",
    "            loss += tf.add_n(cnn_model.losses)\n",
    "    grads = tape.gradient(loss, cnn_model.trainable_variables)\n",
    "    vg = [(g, v) for g, v in zip(grads, cnn_model.trainable_variables) if g is not None]\n",
    "    if vg:\n",
    "        optimizer.apply_gradients(vg)\n",
    "    return loss, [g for g, _ in vg]\n",
    "\n",
    "def run_rl_controlled_cnn_step(cnn_model, optimizer, loss_fn, rl_env, ddqn_agent, policy, current_state, x_batch, y_batch, store_transition=True):\n",
    "    if current_state is None or not isinstance(current_state, np.ndarray):\n",
    "        raise ValueError(\"Invalid state\")\n",
    "    action = ddqn_agent.get_action(current_state, policy)\n",
    "    lr_prev = rl_env.get_current_lr()\n",
    "    loss_cnn, grads = cnn_train_step(cnn_model, optimizer, loss_fn, x_batch, y_batch)\n",
    "    new_lr, reward = rl_env.apply_action_and_get_reward(action, loss_cnn)\n",
    "    next_state = rl_env.calculate_state(loss_cnn, grads)\n",
    "    if store_transition:\n",
    "        ddqn_agent.store_transition(current_state, action, reward, next_state, False)\n",
    "    return next_state, reward, loss_cnn, lr_prev\n",
    "\n",
    "def train_cnn_with_rl(main_cnn, game_cnn, optimizer, loss_fn, ddqn_agent, rl_env, data_loader, config):\n",
    "    epochs = config.get('epochs', 10)\n",
    "    step4game = config.get('step4game', 5)\n",
    "    policy = EpsilonGreedyPolicy()\n",
    "    history = {'epoch': [], 'step': [], 'main_cnn_loss': [], 'game_cnn_loss': [], 'q_loss': [], 'reward': [], 'lr': []}\n",
    "    training_steps = 0\n",
    "    first = False\n",
    "    for epoch in range(epochs):\n",
    "        sum_main = sum_game = sum_q = sum_reward = 0\n",
    "        updates = steps = 0\n",
    "        pbar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        for x_batch, y_batch in pbar:\n",
    "            x_batch = tf.convert_to_tensor(x_batch, tf.float32)\n",
    "            y_batch = tf.convert_to_tensor(y_batch)\n",
    "            game_cnn.set_weights(main_cnn.get_weights())\n",
    "            if not first:\n",
    "                loss0, grads0 = cnn_train_step(main_cnn, optimizer, loss_fn, x_batch, y_batch)\n",
    "                rl_env.reset()\n",
    "                state = rl_env.calculate_state(loss0, grads0)\n",
    "                first = True\n",
    "            else:\n",
    "                state = state\n",
    "            gs = state.copy()\n",
    "            gl = 0\n",
    "            for _ in range(step4game):\n",
    "                action = ddqn_agent.get_action(gs, policy)\n",
    "                lg, gg = cnn_train_step(game_cnn, optimizer, loss_fn, x_batch, y_batch)\n",
    "                gl += float(lg.numpy())\n",
    "                _, rg = rl_env.apply_action_and_get_reward(action, lg)\n",
    "                ns = rl_env.calculate_state(lg, gg)\n",
    "                ddqn_agent.store_transition(gs, action, rg, ns, False)\n",
    "                gs = ns\n",
    "            avg_game = gl/step4game\n",
    "            ql = ddqn_agent.update() or 0\n",
    "            ml = mr = 0\n",
    "            ms = state.copy()\n",
    "            lr0 = rl_env.get_current_lr()\n",
    "            for _ in range(step4game):\n",
    "                ms, rm, lm, lr0 = run_rl_controlled_cnn_step(main_cnn, optimizer, loss_fn, rl_env, ddqn_agent, policy, ms, x_batch, y_batch)\n",
    "                ml += float(lm.numpy())\n",
    "                mr += rm\n",
    "            avg_main = ml/step4game\n",
    "            avg_reward = mr/step4game\n",
    "            state = ms\n",
    "            sum_main += avg_main\n",
    "            sum_game += avg_game\n",
    "            sum_q += ql\n",
    "            sum_reward += avg_reward\n",
    "            updates += 1 if ql else 0\n",
    "            steps += 1\n",
    "            training_steps += 1\n",
    "            pbar.set_postfix({'main_loss':f\"{avg_main:.3f}\", 'q_loss':f\"{ql:.3f}\", 'reward':f\"{avg_reward:.3f}\", 'lr':f\"{lr0:.6f}\"})\n",
    "        history['epoch'].append(epoch+1)\n",
    "        history['step'].append(training_steps)\n",
    "        history['main_cnn_loss'].append(sum_main/steps)\n",
    "        history['game_cnn_loss'].append(sum_game/steps)\n",
    "        history['q_loss'].append(sum_q/updates if updates else 0)\n",
    "        history['reward'].append(sum_reward/steps)\n",
    "        history['lr'].append(rl_env.get_current_lr())\n",
    "    return main_cnn, ddqn_agent, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'step_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[275], line 8\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1) Eğitimden gelen per-step LR (liste veya array)\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# step_lr = np.array(step_lr)  \u001b[39;00m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Eğer sadece RL ajanınızın eğrisini çizmek istiyorsanız, aşağıda (g) olarak ekleyebilirsiniz.\u001b[39;00m\n",
      "\u001b[0;32m----> 8\u001b[0m n_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mstep_lr\u001b[49m)\n",
      "\u001b[1;32m      9\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(n_steps)\n",
      "\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2) Baseline schedule fonksiyonları\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'step_lr' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Eğitimden gelen per-step LR (liste veya array)\n",
    "# step_lr = np.array(step_lr)  \n",
    "# Eğer sadece RL ajanınızın eğrisini çizmek istiyorsanız, aşağıda (g) olarak ekleyebilirsiniz.\n",
    "\n",
    "n_steps = len(step_lr)\n",
    "t = np.arange(n_steps)\n",
    "\n",
    "# 2) Baseline schedule fonksiyonları\n",
    "def exp_decay(lr0, lr_end, T):\n",
    "    return lr0 * (lr_end / lr0) ** (t / T)\n",
    "\n",
    "def quad_decay(lr0, lr_end, T):\n",
    "    return lr0 * (1 - (t / T)**2)\n",
    "\n",
    "def lin_decay(lr0, lr_end, T):\n",
    "    return lr0 * (1 - t / T)\n",
    "\n",
    "def hyper_decay(lr0, k):\n",
    "    # burada k küçülme katsayısı (ör. k=1e-3). Ayarlayın.\n",
    "    return lr0 / (1 + k * t)\n",
    "\n",
    "def cyclic_decay(lr0, lr_max, T):\n",
    "    # yarı dalga kosinüs\n",
    "    return lr0 + (lr_max - lr0) * 0.5 * (1 - np.cos(2 * np.pi * t / T))\n",
    "\n",
    "def increase_to_max(lr0, lr_max, T):\n",
    "    return lr0 + (lr_max - lr0) * (t / T)\n",
    "\n",
    "# 3) Parametreler\n",
    "lr0      = step_lr[0]                    # baştaki\n",
    "lr_end   = lr0 * 0.01                    # örnek son değer\n",
    "lr_max   = lr0 * 10                      # örnek maksimum\n",
    "T        = float(n_steps - 1)\n",
    "k_hyper  = 1e-3                          # hyperbolik sabit, deneyin\n",
    "\n",
    "# Hesaplayalım\n",
    "curves = {\n",
    "    \"(a) Exponential\": exp_decay(lr0, lr_end, T),\n",
    "    \"(b) Quadratic\":   quad_decay(lr0, lr_end, T),\n",
    "    \"(c) Linear\":      lin_decay(lr0, lr_end, T),\n",
    "    \"(d) Hyperbolic\":  hyper_decay(lr0, k_hyper),\n",
    "    \"(e) Cyclic\":      cyclic_decay(lr0, lr_max, T),\n",
    "    \"(f) Increase\":    increase_to_max(lr0, lr_max, T),\n",
    "    # dilerseniz kendi ajan eğrisini (g) olarak da ekleyin:\n",
    "    # \"(g) RL Agent\": step_lr\n",
    "}\n",
    "\n",
    "# 4) Plot\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8), sharex=True, sharey=True)\n",
    "for ax, (title, lr_curve) in zip(axs.flatten(), curves.items()):\n",
    "    ax.plot(t, np.log10(lr_curve))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Training Steps\")\n",
    "    ax.set_ylabel(\"Learning Rate (Log₁₀)\")\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def s_transform(signal: np.ndarray, fs: float, f_min: float = 5, f_max: float = None, n_freqs: int = 100) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    n_samples = len(signal)\n",
    "    if n_samples == 0:\n",
    "        return np.empty((n_freqs, 0), dtype=complex), np.array([]), np.array([])\n",
    "\n",
    "    time_vector = np.arange(n_samples) / fs\n",
    "    if f_max is None:\n",
    "        f_max = fs / 2\n",
    "    if f_min >= f_max:\n",
    "        freq_vector = np.linspace(f_min, f_max, n_freqs) if n_freqs > 0 else np.array([])\n",
    "        return np.zeros((n_freqs, n_samples), dtype=complex), time_vector, freq_vector\n",
    "\n",
    "    freq_vector = np.linspace(f_min, f_max, n_freqs)\n",
    "    signal_fft = fft(signal)\n",
    "    fft_freqs = np.fft.fftfreq(n_samples, 1.0/fs)\n",
    "    s_matrix = np.zeros((n_freqs, n_samples), dtype=complex)\n",
    "\n",
    "    for i, f in enumerate(freq_vector):\n",
    "        if f < 1e-9:\n",
    "            continue\n",
    "        sigma_f = f / (2.0 * np.pi)\n",
    "        gauss_win_freq = np.exp(-0.5 * ((fft_freqs - f) / sigma_f)**2)\n",
    "        gauss_win_freq_neg = np.exp(-0.5 * ((fft_freqs + f) / sigma_f)**2)\n",
    "        st_freq = signal_fft * (gauss_win_freq + gauss_win_freq_neg)\n",
    "        s_matrix[i, :] = ifft(st_freq)\n",
    "\n",
    "    return s_matrix, time_vector, freq_vector\n",
    "\n",
    "\n",
    "def load_cwru_signal(file_path: str, signal_key: str) -> Optional[np.ndarray]:\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "    try:\n",
    "        data = loadmat(file_path)\n",
    "        keys = [k for k in data.keys() if signal_key in k]\n",
    "        if not keys:\n",
    "            numeric_keys = [k for k, v in data.items() if isinstance(v, np.ndarray) and v.ndim >= 1 and np.issubdtype(v.dtype, np.number)]\n",
    "            if not numeric_keys:\n",
    "                 return None\n",
    "            keys = sorted(numeric_keys, key=lambda k: len(data[k]), reverse=True)\n",
    "        signal = data[keys[0]].flatten().astype(np.float32)\n",
    "        return signal\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def segment_signal(signal: np.ndarray, segment_length: int, overlap: int = 0) -> List[np.ndarray]:\n",
    "    if signal is None or len(signal) < segment_length:\n",
    "        return []\n",
    "    step = segment_length - overlap\n",
    "    if step <= 0:\n",
    "        step = 1\n",
    "    start_indices = range(0, len(signal) - segment_length + 1, step)\n",
    "    return [signal[i : i + segment_length] for i in start_indices]\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(config: dict) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[tuple], Optional[int]]:\n",
    "    dataset_path = config.get('dataset_path', '/Users/kasimesen/Desktop/Kodlar/Python/12K_Drive_End_Bearing_Fault')\n",
    "    dataset_name = config.get('dataset_name', 'CWRU')\n",
    "    segment_length = config.get('segment_length', 1024)\n",
    "    overlap = config.get('overlap', 0)\n",
    "    fs = config.get('fs', 12000)\n",
    "    signal_key = config.get('signal_key', 'DE_time')\n",
    "    f_min = config.get('f_min', 5)\n",
    "    f_max = config.get('f_max', fs / 3)\n",
    "    n_freqs = config.get('n_freqs', 64)\n",
    "    target_height = config.get('input_height', n_freqs)\n",
    "    target_width = config.get('input_width', segment_length)\n",
    "\n",
    "    all_processed_samples = []\n",
    "    all_labels = []\n",
    "    class_map = {}\n",
    "    current_label = 0\n",
    "    base_path = os.path.join(dataset_path, dataset_name)\n",
    "    if not os.path.isdir(base_path):\n",
    "        return None, None, None, None\n",
    "\n",
    "    try:\n",
    "        condition_folders = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
    "        for condition_folder in condition_folders:\n",
    "            if condition_folder not in class_map:\n",
    "                 class_map[condition_folder] = current_label\n",
    "                 current_label += 1\n",
    "            label = class_map[condition_folder]\n",
    "            filenames = sorted([f for f in os.listdir(os.path.join(base_path, condition_folder)) if f.lower().endswith(\".mat\")])\n",
    "            for filename in filenames:\n",
    "                signal = load_cwru_signal(os.path.join(base_path, condition_folder, filename), signal_key)\n",
    "                if signal is None: continue\n",
    "                for segment in segment_signal(signal, segment_length, overlap):\n",
    "                    s_matrix, _, _ = s_transform(segment, fs=fs, f_min=f_min, f_max=f_max, n_freqs=n_freqs)\n",
    "                    s_magnitude = np.abs(s_matrix)\n",
    "                    processed_sample = s_magnitude\n",
    "                    if target_height != n_freqs or target_width != segment_length:\n",
    "                        resized_sample = tf.image.resize(processed_sample[..., tf.newaxis], [target_height, target_width], method=tf.image.ResizeMethod.BILINEAR)\n",
    "                        processed_sample = tf.squeeze(resized_sample, axis=-1).numpy()\n",
    "                    all_processed_samples.append(processed_sample)\n",
    "                    all_labels.append(label)\n",
    "        if not all_processed_samples:\n",
    "            return None, None, None, None\n",
    "        X = np.array(all_processed_samples, dtype=np.float32)[..., np.newaxis]\n",
    "        y = np.array(all_labels, dtype=np.int64)\n",
    "        input_shape = X.shape[1:]\n",
    "        num_classes = len(class_map)\n",
    "        return X, y, input_shape, num_classes\n",
    "    except Exception:\n",
    "         return None, None, None, None\n",
    "\n",
    "\n",
    "def get_dataloader(X: np.ndarray, y: np.ndarray, fold_index: int, k_folds: int, batch_size: int, seed: int = 42) -> Tuple[Optional[tf.data.Dataset], Optional[tf.data.Dataset]]:\n",
    "    if X is None or y is None or len(X) != len(y):\n",
    "        return None, None\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "    train_indices, val_indices = list(kf.split(X, y))[fold_index]\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_val, y_val = X[val_indices], y[val_indices]\n",
    "    train_loader = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_loader = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_loader, val_loader\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
